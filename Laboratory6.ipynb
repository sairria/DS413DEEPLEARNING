{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd968376-89cc-4db1-a84e-37d31c505395",
   "metadata": {},
   "source": [
    "**Name:** Airyll Sanchez\n",
    "\n",
    "**Date:** September 30,2025\n",
    "\n",
    "## Laboratory Task # 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b31b5c7e-ab15-4fe7-9418-86781118a91d",
   "metadata": {},
   "source": [
    "![image.png](images/Laboratory6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8396f248-8e71-4e8b-ae03-8ccaf0953015",
   "metadata": {},
   "source": [
    "1. Introduction and SetUp\n",
    "   \n",
    "\n",
    "This notebook implements a deep Convolutional Neural Network (CNN) in PyTorch based on the provided architecture diagram. The network is designed for image classification, taking a 28Ã—28 single-channel image (like MNIST) as input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1d2940-87f4-434c-bacc-a9718a1b01eb",
   "metadata": {},
   "source": [
    "**Imports & Architecture Constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bba4e9bf-1076-4f2e-ba50-3143fd81d672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE CELL 1: Imports and Architecture Constants\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define constants based on the standard input (e.g., MNIST)\n",
    "INPUT_CHANNELS = 1      # (1, 28, 28)\n",
    "INPUT_WIDTH = 28        # 28 pixels\n",
    "DROPOUT_PROB = 0.2      # p = 0.2\n",
    "FINAL_OUTPUT_CLASSES = 10 # Assuming 10 classes (like MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f545a0-6ce9-4ce1-8cf6-143a5c6e767c",
   "metadata": {},
   "source": [
    " **CNN Architecture Conversion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68337fc3-04b0-407a-8e24-d300386cb8c3",
   "metadata": {},
   "source": [
    "We will use the nn.Module class to define the network, calculating the exact spatial dimensions at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68db1559-1adb-4041-b83d-0ebe6742f9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Instantiation Complete.\n",
      "CustomDeepCNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fcn1): Linear(in_features=12544, out_features=1000, bias=True)\n",
      "  (fcn2): Linear(in_features=1000, out_features=500, bias=True)\n",
      "  (fcn3): Linear(in_features=500, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#  PyTorch CNN Class Definition\n",
    "\n",
    "class CustomDeepCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the CNN architecture defined in Laboratory Task #6.\n",
    "    The architecture uses 4 Conv layers, 2 MaxPool layers, Dropout, and 3 Fully Connected layers.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CustomDeepCNN, self).__init__()\n",
    "\n",
    "        # --- CONVOLUTIONAL LAYERS ---\n",
    "        # The input channel sizes are inferred from the desired output shapes:\n",
    "        # (32, 32, 28, 28) -> Output Channels = 32\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1) \n",
    "        \n",
    "        # MaxPool1 output is (32, 14, 14) due to kernel 2, stride 2, padding 0.\n",
    "        # But the instructions specify padding=1, which is unusual for a simple 28->14 reduction.\n",
    "        # We assume the intent is 28->14, which requires stride=2, kernel=2, and padding=0 (standard pooling)\n",
    "        # OR padding=1, which requires a calculation: (28 - 2 + 2*1)/2 + 1 = 15. The shape (32, 32, 28, 28) \n",
    "        # is also confusing. We will interpret MaxPool1 as STANDARD 28->14 downsampling.\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        # The subsequent Conv layers increase the depth (channels). We follow the pattern: 32 -> 64 -> 128 -> 256\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # MaxPool2: kernel=(2,2), stride=2, padding=0. Reduces spatial size by half.\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # Dropout Layer\n",
    "        self.dropout = nn.Dropout(p=DROPOUT_PROB)\n",
    "\n",
    "        # --- FULLY CONNECTED LAYERS (FCN) ---\n",
    "        # Calculate the size before FCN1 (FLATTEN input)\n",
    "        # 1. After Conv1/MaxPool1: 28x28 -> 14x14 (32 channels)\n",
    "        # 2. After Conv2/Conv3/Conv4: 14x14 -> 14x14 (256 channels)\n",
    "        # 3. After MaxPool2: 14x14 -> 7x7 (256 channels)\n",
    "        \n",
    "        FLATTEN_SIZE = 256 * 7 * 7  # 256 * 49 = 12,544\n",
    "\n",
    "        # FCN1: input = 12,544, output = 1000\n",
    "        self.fcn1 = nn.Linear(FLATTEN_SIZE, 1000)\n",
    "        \n",
    "        # FCN2: input = 1000, output = 500\n",
    "        self.fcn2 = nn.Linear(1000, 500)\n",
    "        \n",
    "        # FCN3: input = 500, output = FINAL_OUTPUT_CLASSES (10)\n",
    "        self.fcn3 = nn.Linear(500, FINAL_OUTPUT_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Conv Block 1\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x) \n",
    "        \n",
    "        # 2. Conv Block 2-4\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # 3. Dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 4. Flatten: prepare for FCN layers\n",
    "        x = x.view(x.size(0), -1) # x.size(0) is the batch size\n",
    "        \n",
    "        # 5. Fully Connected Layers\n",
    "        x = F.relu(self.fcn1(x))\n",
    "        x = F.relu(self.fcn2(x))\n",
    "        \n",
    "        # FCN3 (No ReLU/Activation on the final layer before SoftMax is standard)\n",
    "        x = self.fcn3(x)\n",
    "        \n",
    "        # 6. Final Activation (SoftMax for probability output)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = CustomDeepCNN()\n",
    "print(\"Model Instantiation Complete.\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141fb7f3-f32f-44ff-8fe9-6d3e68df1628",
   "metadata": {},
   "source": [
    "**Architecture Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1b574e-19b7-4cc2-890c-dd8260cc9b7f",
   "metadata": {},
   "source": [
    "Architectural Documentation and Layer-by-Layer Calculation\n",
    "\n",
    "The model was constructed by following the layer sequence and parameters provided. The spatial dimensions (W x H) and channel depth (C) are calculated below:\n",
    "\n",
    "**A. Initial State**\n",
    "\n",
    "* **Input:** $(C_{in}, W, H) = (1, 28, 28)$\n",
    "\n",
    "**B. Layer Computations**\n",
    "\n",
    "| Layer | Input Shape | Operation | Output Formula $\\left( \\frac{W - F + 2P}{S} + 1 \\right)$ | Output Shape |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **Conv1 (ReLU)** | $(1, 28, 28)$ | $K=3, S=1, P=1, C_{out}=32$ | $\\frac{28 - 3 + 2(1)}{1} + 1 = 28$ | $(32, 28, 28)$ |\n",
    "| **MaxPool1** | $(32, 28, 28)$ | $K=2, S=2, P=0$ (Assumed P=0 for 28->14) | $\\frac{28 - 2 + 2(0)}{2} + 1 = 14$ | $(32, 14, 14)$ |\n",
    "| **Conv2 (ReLU)** | $(32, 14, 14)$ | $K=3, S=1, P=1, C_{out}=64$ | $\\frac{14 - 3 + 2(1)}{1} + 1 = 14$ | $(64, 14, 14)$ |\n",
    "| **Conv3 (ReLU)** | $(64, 14, 14)$ | $K=3, S=1, P=1, C_{out}=128$ | $\\frac{14 - 3 + 2(1)}{1} + 1 = 14$ | $(128, 14, 14)$ |\n",
    "| **Conv4 (ReLU)** | $(128, 14, 14)$ | $K=3, S=1, P=1, C_{out}=256$ | $\\frac{14 - 3 + 2(1)}{1} + 1 = 14$ | $(256, 14, 14)$ |\n",
    "| **MaxPool2** | $(256, 14, 14)$ | $K=2, S=2, P=0$ | $\\frac{14 - 2 + 2(0)}{2} + 1 = 7$ | $(256, 7, 7)$ |\n",
    "| **Flatten** | $(256, 7, 7)$ | $256 \\times 7 \\times 7 = 12,544$ | N/A | $(12,544)$ |\n",
    "\n",
    "**C. Fully Connected Layers**\n",
    "\n",
    "* **FCN1:** $\\text{Input}=12,544$, $\\text{Output}=1000$\n",
    "* **FCN2:** $\\text{Input}=1000$, $\\text{Output}=500$\n",
    "* **FCN3:** $\\text{Input}=500$, $\\text{Output}=10$ (Final classification layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01facbb5-aeb2-4284-8054-24858dbb29ba",
   "metadata": {},
   "source": [
    "**Functionality Demonstration (MNIST Data)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33194cbe-a7aa-4b76-9f4c-2da564ff7f62",
   "metadata": {},
   "source": [
    "     We will use the MNIST dataset to confirm that the implemented CustomDeepCNN class correctly processes images through all its layers and outputs the required final tensor shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8912cbe1-b5d3-4bf8-8afb-9e7979050fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Data Setup and Forward Pass Check\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "382fff7b-6e67-42d9-b476-66d9da4256ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Batch Shape: torch.Size([4, 1, 28, 28])\n",
      "-----------------------------------\n",
      "Final Output Tensor Shape (Batch x Classes): torch.Size([4, 10])\n",
      "Verification: Initial image shape: (1, 28, 28)\n",
      "Verification: Flattened feature map size: 12544\n",
      "Verification: Final output classes: 10\n"
     ]
    }
   ],
   "source": [
    "# 1. Load MNIST Data\n",
    "transform = transforms.ToTensor()\n",
    "test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=4, shuffle=True)\n",
    "\n",
    "# 2. Get a batch of images for testing\n",
    "images, labels = next(iter(test_loader))\n",
    "\n",
    "print(f\"Input Batch Shape: {images.shape}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# 3. Perform a forward pass on the architecture\n",
    "model.eval() # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output_tensor = model(images)\n",
    "    \n",
    "# 4. Display the output shape\n",
    "print(f\"Final Output Tensor Shape (Batch x Classes): {output_tensor.shape}\")\n",
    "\n",
    "# 5. Display initial and final shapes for verification\n",
    "print(f\"Verification: Initial image shape: (1, 28, 28)\")\n",
    "print(f\"Verification: Flattened feature map size: {256 * 7 * 7}\")\n",
    "print(f\"Verification: Final output classes: {output_tensor.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500293a9-4714-4033-8a40-43e9656e633e",
   "metadata": {},
   "source": [
    "**Visualization: Feature Map Inspection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532f84bd-e800-40ee-95c8-1a4840877d91",
   "metadata": {},
   "source": [
    "     The most effective visualization to check a CNN's functionality is to inspect the outputs of the intermediate layers (feature maps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38cc6dab-ee18-496a-b291-86410eb5e3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x200 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CODE CELL 4: Visualization of Layer 1 Feature Maps\n",
    "\n",
    "def visualize_feature_maps(model, input_image, layer_name, num_maps=8):\n",
    "    \"\"\"\n",
    "    Visualizes a selection of feature maps from a specified convolutional layer.\n",
    "    \"\"\"\n",
    "    # 1. Create a partial model (sub-model) that runs up to the target layer\n",
    "    # target_layer = getattr(model, layer_name) # This line is not needed for the current function logic\n",
    "    \n",
    "    # We will manually run the first part of the forward pass up to Conv1\n",
    "    x = F.relu(model.conv1(input_image))\n",
    "    \n",
    "    # 2. Extract feature maps and detach from graph\n",
    "    # --- FIX APPLIED HERE: Added .detach() ---\n",
    "    feature_maps = x[0].detach().cpu().numpy() # Get the feature maps for the first image in the batch\n",
    "    # ----------------------------------------\n",
    "    \n",
    "    # 3. Plotting\n",
    "    fig, axs = plt.subplots(1, num_maps, figsize=(15, 2))\n",
    "    fig.suptitle(f'Feature Maps from {layer_name} (First {num_maps} of {feature_maps.shape[0]})', fontsize=12)\n",
    "    \n",
    "    for i in range(num_maps):\n",
    "        axs[i].imshow(feature_maps[i], cmap='gray')\n",
    "        axs[i].axis('off')\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "# --- Execution (Remaining code unchanged) ---\n",
    "# Use the first image from the loaded batch\n",
    "single_image = images[0].unsqueeze(0) \n",
    "\n",
    "# Display the original input image\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(single_image[0, 0].cpu().numpy(), cmap='gray')\n",
    "plt.title(f'Original Digit: {labels[0].item()}')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Visualize the feature maps after the first convolutional layer\n",
    "visualize_feature_maps(model, single_image, 'conv1', num_maps=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b750983a-0f2f-499c-b413-8dc7d8892de8",
   "metadata": {},
   "source": [
    "**Key Takeaway: Functionality Check**\n",
    "\n",
    "The demonstration confirms the following:\n",
    "\n",
    "1.  **Correct Dimensions:** The forward pass successfully transforms the initial $(1, 28, 28)$ input into the final $(4, 10)$ output (Batch Size 4, 10 Classes), proving the layer geometry and the final $\\text{Flatten} \\rightarrow \\text{FCN}$ calculations are correct.\n",
    "2.  **Feature Extraction:** The **Feature Map Visualization** shows how the initial convolutional layer extracts basic features (edges, corners) from the raw pixel data, converting the single input channel into 32 distinct feature representations. This verifies that the deep architecture is actively processing and transforming the image information as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d011e74-3ad6-4e15-8391-4411d4d07986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sairria)",
   "language": "python",
   "name": "sairria"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
